% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/individual_variable_importance.R
\name{individual_variable_importance}
\alias{individual_variable_importance}
\alias{individual_variable_importance.explainer}
\alias{individual_variable_importance.default}
\title{KernelExplainer}
\usage{
individual_variable_importance(x, ...)

\method{individual_variable_importance}{explainer}(x, new_observation,
  method = "KernelSHAP", ...)

\method{individual_variable_importance}{default}(x, data, predict_function,
  new_observation, label, method = "KernelSHAP", nsamples = 100, ...)
}
\arguments{
\item{x}{a model to be explained, or an explainer created with function \code{\link[DALEX]{explain}}.}

\item{...}{other parameters.}

\item{new_observation}{an observation/observations to be explained. Required for local/instance level
explainers. Columns in should correspond to columns in the data argument.}

\item{method}{an estimation method of SHAP values. Currently the only availible is `KernelSHAP`.}

\item{data}{validation dataset. Used to determine univariate distributions, calculation of quantiles,
correlations and so on. It will be extracted from `x` if it’s an explainer.}

\item{predict_function}{predict function that operates on the model `x`. Since the model is a black box,
the `predict_function` is the only interface to access values from the model. It should be a function that
takes at least a model `x` and data and returns vector of predictions. If model response has more than
a single number (like multiclass models) then this function should return a marix/data.frame of the size
`m` x `d`, where `m` is the number of observations while `d` is the dimensionality of model response.
It will be extracted from `x` if it’s an explainer.}

\item{label}{name of the model. By default it’s extracted from the class attribute of the model}

\item{nsamples}{number of samples}
}
\description{
KernelExplainer
}
